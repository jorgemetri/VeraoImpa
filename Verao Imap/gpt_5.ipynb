{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27509297",
   "metadata": {},
   "source": [
    "# 5 - GPT\n",
    "\n",
    "Nosso objetivo é usar redes neurais para gerar algo que se pareça com português. Mais especificamente, algo que se pareça com um português específico --- aquele usado por Machado de Assis.\n",
    "\n",
    "O código desse notebook é adaptado do repositório `nn-zero-to-hero`, de Andrej Karpathy. Os notebooks correspondentes (em inglês) podem ser encontrados na seguinte URL: https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/.\n",
    "\n",
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7638acb3",
   "metadata": {},
   "source": [
    "Agora, munidos do nosso conhecimento de redes neurais, vamos testar uma arquitetura específica: GPT. Essa rede, por sua vez, é a base do ChatGPT.\n",
    "\n",
    "Pra ser mais preciso, o GPT mistura duas coisas: (i) um encoder, capaz de entender uma noção de contexto (e.g., um usuário perguntando quem é o atual presidente do Brasil); e (ii) um decoder, capaz de gerar texto respeitando esse pedido (e.g., uma resposta coerente como \"O atual presidente é ...\"). Considerando nossa missão de gerar português baseado em Machado de Assis, vamos nos preocupar apenas com (ii) hoje.\n",
    "\n",
    "A estrutura-base do GPT, um transformer, está especificada abaixo.\n",
    "\n",
    "![](https://substack-post-media.s3.amazonaws.com/public/images/d9a0766d-2e52-4af0-96c5-3e07a30d6ecb_1868x1130.png)\n",
    "\n",
    "Vamos implementar, em código, cada um dos passos do decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64e0941",
   "metadata": {},
   "source": [
    "## 5.1 Criando os inputs e outputs para a rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "198398e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57c29ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/gpt/processed/machado-all.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: entra string, saem inteiros\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: entram inteiros, sai uma string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8f1a0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 7, 19, 18, 24, 19,  1,  0,  7, 19, 18, 24, 19, 23,  0, 10, 16, 25, 17,\n",
      "        13, 18,  9, 18, 23,  9, 23,  1,  0,  7, 19, 18, 24, 19, 23,  0, 10, 16,\n",
      "        25, 17, 13, 18,  9, 18, 23,  9, 23,  0, 24,  9, 28, 24, 19,  2, 10, 19,\n",
      "        18, 24,  9,  0, 19,  6, 22,  5,  0,  7, 19, 17, 20, 16,  9, 24,  5,  1,\n",
      "         0, 17,  5,  7, 12,  5,  8, 19,  0,  8,  9,  0,  5, 23, 23, 13, 23,  1,\n",
      "         0, 26, 19, 16,  3,  0, 13, 13,  1,  0])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f729830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits de treino e validação\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # os primeiros 90% serão treino\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1887d9b4",
   "metadata": {},
   "source": [
    "A ideia vai ser dar pedaços de texto para a rede neural, com o objetivo de prever o caractere seguinte. Para isso, vamos fornecer um certo bloco de texto para a rede: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "785b0789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([19, 18, 24, 19,  1,  0,  7, 19])\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "print(train_data[1:block_size+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa31247",
   "metadata": {},
   "source": [
    "Mas repare que existem múltiplos exemplos dentro desse bloco. Queremos que a rede saiba prever o próximo caractere quando apenas um caractere de contexto é fornecido, ou quando o bloco inteiro é fornecido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc482bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contexto é tensor([7]) para prever 19\n",
      "Contexto é tensor([ 7, 19]) para prever 18\n",
      "Contexto é tensor([ 7, 19, 18]) para prever 24\n",
      "Contexto é tensor([ 7, 19, 18, 24]) para prever 19\n",
      "Contexto é tensor([ 7, 19, 18, 24, 19]) para prever 1\n",
      "Contexto é tensor([ 7, 19, 18, 24, 19,  1]) para prever 0\n",
      "Contexto é tensor([ 7, 19, 18, 24, 19,  1,  0]) para prever 7\n",
      "Contexto é tensor([ 7, 19, 18, 24, 19,  1,  0,  7]) para prever 19\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    print(f\"Contexto é {x[:t+1]} para prever {y[t]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0afd045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temos 4 exemplos num batch, cada um com tamanho 8\n",
      "\n",
      " - Input (x):\n",
      "torch.Size([4, 8])\n",
      "tensor([[ 7, 12,  9, 13, 19, 23,  0,  8],\n",
      "        [ 0, 13, 24, 32, 16, 13,  5,  0],\n",
      "        [ 5, 26, 13, 16, 12,  5,  3,  0],\n",
      "        [ 5, 23, 23, 13, 17,  1,  0,  9]])\n",
      "\n",
      "-Alvo (y):\n",
      "torch.Size([4, 8])\n",
      "tensor([[12,  9, 13, 19, 23,  0,  8,  9],\n",
      "        [13, 24, 32, 16, 13,  5,  0,  8],\n",
      "        [26, 13, 16, 12,  5,  3,  0,  8],\n",
      "        [23, 23, 13, 17,  1,  0,  9, 22]])\n",
      "\n",
      "--------\n",
      "Ou seja, isso pode ser entendido como:\n",
      "\n",
      "Contexto é [7] para prever 12\n",
      "Contexto é [7, 12] para prever 9\n",
      "Contexto é [7, 12, 9] para prever 13\n",
      "Contexto é [7, 12, 9, 13] para prever 19\n",
      "Contexto é [7, 12, 9, 13, 19] para prever 23\n",
      "Contexto é [7, 12, 9, 13, 19, 23] para prever 0\n",
      "Contexto é [7, 12, 9, 13, 19, 23, 0] para prever 8\n",
      "Contexto é [7, 12, 9, 13, 19, 23, 0, 8] para prever 9\n",
      "Contexto é [0] para prever 13\n",
      "Contexto é [0, 13] para prever 24\n",
      "Contexto é [0, 13, 24] para prever 32\n",
      "Contexto é [0, 13, 24, 32] para prever 16\n",
      "Contexto é [0, 13, 24, 32, 16] para prever 13\n",
      "Contexto é [0, 13, 24, 32, 16, 13] para prever 5\n",
      "Contexto é [0, 13, 24, 32, 16, 13, 5] para prever 0\n",
      "Contexto é [0, 13, 24, 32, 16, 13, 5, 0] para prever 8\n",
      "Contexto é [5] para prever 26\n",
      "Contexto é [5, 26] para prever 13\n",
      "Contexto é [5, 26, 13] para prever 16\n",
      "Contexto é [5, 26, 13, 16] para prever 12\n",
      "Contexto é [5, 26, 13, 16, 12] para prever 5\n",
      "Contexto é [5, 26, 13, 16, 12, 5] para prever 3\n",
      "Contexto é [5, 26, 13, 16, 12, 5, 3] para prever 0\n",
      "Contexto é [5, 26, 13, 16, 12, 5, 3, 0] para prever 8\n",
      "Contexto é [5] para prever 23\n",
      "Contexto é [5, 23] para prever 23\n",
      "Contexto é [5, 23, 23] para prever 13\n",
      "Contexto é [5, 23, 23, 13] para prever 17\n",
      "Contexto é [5, 23, 23, 13, 17] para prever 1\n",
      "Contexto é [5, 23, 23, 13, 17, 1] para prever 0\n",
      "Contexto é [5, 23, 23, 13, 17, 1, 0] para prever 9\n",
      "Contexto é [5, 23, 23, 13, 17, 1, 0, 9] para prever 22\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "batch_size = 4  # Quantas sequências independentes serão processadas em paralelo?\n",
    "block_size = 8  # Qual é o maior contexto disponível numa sequência?\n",
    "\n",
    "print(f\"Temos {batch_size} exemplos num batch, cada um com tamanho {block_size}\\n\")\n",
    "\n",
    "def get_batch(split):\n",
    "    # Gere um único batch de dados com inputs x e alvos y.\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))  # Um índice aleatório nos dados.\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(' - Input (x):')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('\\n-Alvo (y):')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('\\n--------\\nOu seja, isso pode ser entendido como:\\n')\n",
    "\n",
    "for b in range(batch_size): # Dimensão de batch.\n",
    "    for t in range(block_size): # Dimensão de tempo.\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"Contexto é {context.tolist()} para prever {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48369b2",
   "metadata": {},
   "source": [
    "## 5.2 Criando um modelo bigrama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879028d6",
   "metadata": {},
   "source": [
    "Vamos começar com um modelo de bigramas, em que dado o caractere atual, queremos prever o seguinte. Ou seja, o contexto tem sempre tamanho 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9c453bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Cada linha da token_embedding_table diz os 42 logits do próximo caractere dado o atual.\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # Tanto idx quanto targets são tensores de dimensão (B, T).\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c4c6063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 42])\n"
     ]
    }
   ],
   "source": [
    "model = LanguageModel(vocab_size)\n",
    "out = model(xb, yb)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbac193",
   "metadata": {},
   "source": [
    "Ou seja, para cada um dos 8 caracteres nos 4 exemplos no batch, nós fazemos a previsão de qual é a probabilidade de cada um dos 42 caracteres existentes ser o próximo caractere na sequência. Ou seja, nossas dimensões são:\n",
    "- batch size `b`: 4 (quantos exemplos temos num batch)\n",
    "- block size `t`: 8 (quantas previsões em sequência temos em cada exemplo)\n",
    "- characters `c`: 42 (quantos caracteres podem ser escolhidos na sequência)\n",
    "\n",
    "E $x_b \\in \\mathbb{R}^{4 \\times 8}$ diz quem é o último caractere em cada exemplo e elemento da sequência. Também, $y_b \\in \\mathbb{R}^{4 \\times 8}$ diz quem é, de fato, o caractere seguinte. Nós estamos usando um embedding que dá a probabilidade de cada um dos $42$ possíveis caracteres serem o caractere seguinte. Logo, nosso `logits` tem dimensão $\\mathbb{R}^{4 \\times 8 \\times 42}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3095ee30",
   "metadata": {},
   "source": [
    "Naturalmente, é preciso escolher a matrix `token_embedding_table` de maneira ótima, para que as probabilidades atribuídas a cada caractere façam sentido (e.g., 'xm' é incomum, 'de' é comum). Para isso, vamos adicionar uma perda ao nosso modelo --- em particular, vamos usar a perda logística (ou \"entropia cruzada\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16fcbcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Cada linha da token_embedding_table diz os 42 logits do próximo caractere dado o atual.\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # Tanto idx quanto targets são tensores de dimensão (B, T).\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "        \n",
    "# ------------------------------------------------------------------------------\n",
    "        # Para gerar caracteres numa sequência, como na função generate abaixo, não temos targets;\n",
    "        # nesse caso, também não há perda.\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        # Para a perda F.cross_entropy, o PyTorch espera que logits tenha C na segunda dimensão.\n",
    "        # Podemos resolver isso rearranjando os logits em (B*T, C) e os targets de maneira correspondente em (B*T)\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx é um tensor (B, T) que fornece o contexto atual.\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Obtenha previsões\n",
    "            logits, loss = self(idx)\n",
    "            # Como nosso modelo é um bigrama, dado o contexto, apenas o último caractere dá os logits\n",
    "            # do possível caractere subsequente.\n",
    "            logits = logits[:, -1, :]  # (B, C)\n",
    "            # Obtenha as probabilidades de cada caractere tirando o softmax dos logits.\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # Amostre dessas probabildades\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # Adicione o índice sorteado ao nosso contexto atual, aumentando a dimensão da sequência.\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7544857c",
   "metadata": {},
   "source": [
    "A função `generate` acima é um pouco dispendiosa. Atualmente, fornecemos todos as sequências que existem dentro de um exemplo (e.g., contexto [0, 26] para prever 13; contexto [0, 26, 13] para prever 9; contexto [0, 26, 13, 9] para prever 22) quando na verdade, por ser um modelo de bigrama, basta o último elemento (e.g., dado [26] prever 13; dado [13] prever 9; dado [9] prever 22). A razão de usar essa função é que, mais abaixo, vamos querer generalizar o modelo de um bigrama para algo que leve em consideração mais contexto. Pela maneira como codificamos `generate`, isso não vai exigir ajustes dessa função."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6abf7d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 42])\n",
      "\n",
      "Perda: 4.414548873901367\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(10)\n",
    "model = LanguageModel(vocab_size)\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape)\n",
    "print(f\"\\nPerda: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffcc19f",
   "metadata": {},
   "source": [
    "Repare que também adicionamos a função `generate`, capaz de usar os pesos aprendidos em `token_embedding_table` para sortear caracteres de acordo com os pesos aprendidos. A variável `max_new_tokens` especifica quantos caracteres devem ser gerados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83d661f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 17, 11, 24, 17, 39, 12,  4, 16, 27, 26, 29, 40, 36,  1, 30,  5, 17,\n",
       "         39,  0, 19, 17,  7, 14, 23, 16,  1, 25, 24, 17, 22, 30, 23, 16, 22, 11,\n",
       "         26, 38, 33, 27, 12, 20, 35, 34, 34, 35, 37, 18,  1, 10, 24, 17, 32, 39,\n",
       "         22, 41, 24,  6,  7, 40, 38, 33, 32, 26, 19, 24, 17, 24,  6, 27, 36, 23,\n",
       "         31, 29,  5,  8, 32, 28, 36,  0,  9,  4, 30, 39, 31,  7, 32, 28, 32,  6,\n",
       "         13,  6, 39, 31, 26,  0,  7, 32, 31, 29, 18]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f6c16a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " põ, cáíâixvçílíópya,e-úzsnea?átmên,eõíãbxájoxcâwszmaãógvírúàg lad?êc,e-ígm,õúlyez.qíõáàêáld.cççá.?éw\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adc49c8",
   "metadata": {},
   "source": [
    "Como não ouve treino, essa amostra é essencialmente aleatória. Na verdade, é pior que isso --- se amostrássemos de maneira uniformemente aleatória, a perda deveria ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63d114cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7376697063446045"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.Tensor([1/vocab_size]).log().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c254b",
   "metadata": {},
   "source": [
    "Como podemos fazer sentido desse valor? A nossa perda aqui é entropia cruzada , e pode ser entendida como a probabilidade de escolhermos o caractere errado. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9256a8c",
   "metadata": {},
   "source": [
    "## 5.3 Treinando um modelo bigrama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad05218",
   "metadata": {},
   "source": [
    "Vamos treinar o modelo bigrama anterior. Para o otimizador, usaremos Adam, que é uma alternativa um pouco mais sofisticada do que o SGD que usamos antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6a55d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6e1c3a",
   "metadata": {},
   "source": [
    "Agora, vamos fazer uma sequência de iterações em que pegamos um batch e damos um passo para otimizar nossa estimativa dos parâmetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c90c40cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: 4.173539161682129\n",
      "Iteration 1000: 3.279672861099243\n",
      "Iteration 2000: 2.8587005138397217\n",
      "Iteration 3000: 2.5384347438812256\n",
      "Iteration 4000: 2.4177489280700684\n",
      "Iteration 5000: 2.3529484272003174\n",
      "Iteration 6000: 2.4395017623901367\n",
      "Iteration 7000: 2.397427797317505\n",
      "Iteration 8000: 2.3495335578918457\n",
      "Iteration 9000: 2.256340742111206\n"
     ]
    }
   ],
   "source": [
    "# torch.manual_seed(10)\n",
    "# model = LanguageModel(vocab_size)\n",
    "# logits, loss = model(xb, yb)\n",
    "\n",
    "batch_size = 32\n",
    "for steps in range(10000): # use mais passos para obter melhores resultados\n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if steps % 1000 == 0:\n",
    "        print(f\"Iteration {steps}: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f6bb602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " o co.ór omamistra neuçandiba ntre e? qussto deesmprdelubíchesqumo, ecirdentemeuvão as ce, ilhao tomo\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf213a8",
   "metadata": {},
   "source": [
    "Ok, isso parece um pouco melhor já.\n",
    "\n",
    "Antes de continuarmos, vai ser útil organizar um pouco melhor o código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17e5278c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteração 0: perda de treino: 4.1996, perda de validação: 4.1961\n",
      "Iteração 300: perda de treino: 2.5214, perda de validação: 2.5614\n",
      "Iteração 600: perda de treino: 2.3496, perda de validação: 2.4021\n",
      "Iteração 900: perda de treino: 2.3193, perda de validação: 2.3719\n",
      "Iteração 1200: perda de treino: 2.3211, perda de validação: 2.3707\n",
      "Iteração 1500: perda de treino: 2.3042, perda de validação: 2.3649\n",
      "Iteração 1800: perda de treino: 2.2948, perda de validação: 2.3618\n",
      "Iteração 2100: perda de treino: 2.3012, perda de validação: 2.3556\n",
      "Iteração 2400: perda de treino: 2.2955, perda de validação: 2.3639\n",
      "Iteração 2700: perda de treino: 2.3038, perda de validação: 2.3581\n",
      "\n",
      " sesa limentar a pradeis amaqunde, qumpfi m doginda nass quisarare mer e nonabeiroça va tum fio asm de m e ptroselha spons. fá do eiz dela a lharitatúnte, rara de é uss, us uerencéitontazum imontyívolapodos pe. toharadasi sulbo mantmospra stuns endenca. o e. canto, toucr aiore u vair de cagupo, vincelempeta qu sto trtes r tia púba. cra ergese he pemorto? uémess mulo fis s, ki m sórbielandisueseu po. aultidosa dianuavo ição itemalho imão ou ez ditares ndore peume mpre fe táguema, da es otede udo p\n"
     ]
    }
   ],
   "source": [
    "# Pacotes\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Hiperparâmetros\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "max_iters = 3000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "\n",
    "torch.manual_seed(10)\n",
    "\n",
    "# Dados\n",
    "with open(\"../data/gpt/processed/machado-all.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# Splits de treino e validação\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # primeiros 90% serão train, resto val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# Função para ler um batch de dados\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# Função para estimar a perda usando vários batches e não um só\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# Nosso modelo bigrama\n",
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        logits = self.token_embedding_table(idx)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = LanguageModel(vocab_size)\n",
    "model = model.to(device)\n",
    "\n",
    "# Otimizador\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Treinamento\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # De tempos em tempos, imprimir a perda\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Iteração {iter}: perda de treino: {losses['train']:.4f}, perda de validação: {losses['val']:.4f}\")\n",
    "\n",
    "    # Amostre um batch de dados\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Dê um passo na otimização\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Gere alguns caracteres de acordo com o modelo\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(\"\\n\" + decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64701c26",
   "metadata": {},
   "source": [
    "Note que acima implementamos duas mudanças técnicas: \n",
    "- A função `estimate_loss` que estima a perda usando não apenas um batch, como antes, mas vários, de modo que a estimativa fica mais suave. Note que a função usa `model.eval()` e `model.train()`, o que deixa o modelo em modo de avaliação ou de treinamento; isso não faz diferença agora, porque nossa rede só usa uma camada de `nn.Embedding`, que se comporta da mesma maneira em treino ou avaliação. Como vimos antes, se incluírmos uma camada de batchnorm, esse não será mais o caso.\n",
    "- O `.to(device)` que coloca o modelo numa GPU, ao invés de CPU. Não vem ao caso entender profundamente porque isso vale a pena além do fato de que, se você tiver uma GPU disponível, o treino provavelmente será muito mais rápido se ele usá-la."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788ec2ee",
   "metadata": {},
   "source": [
    "## 5.4 Positional encoding\n",
    "\n",
    "Vamos começar a criar o decoder. Além do output embedding, que já temos, precisamos de um positional encoding. Como os passos subsequentes vão olhar para o grau de associação entre cada par de caracteres para decidir qual deveria ser o próximo, precisamos ter algum jeito de especificar a sequência que estamos considerando: \"era uma grande pedra\" e \"era uma grande perda\" têm os mesmos pares, mas a ordem dos pares muda o sentido do termo e, portanto, do que é razoável de vir a seguir.\n",
    "\n",
    "Vamos adicionar uma camada `pos_emb` no nosso modelo que codifica a posição de cada par e somá-lo ao nosso embedding dos tokens `tok_emb`. Para deixar que ambos os embeddings assumam uma dimensão arbitrária, e não necessariamente `vocab_size`, vamos introduzir uma última camada linear que leva um tensor de dimensões `(B, T, n_embd)` em `(B, T, vocab_size)`, que é o que precisamos para gerar a probabilidade de cada caractere seguinte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9953081",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 32\n",
    "\n",
    "# Nosso modelo bigrama\n",
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)  # Cada posição recebe um embedding\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B, T, n_embd)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, n_embd)\n",
    "        x = tok_emb + pos_emb  # (B, T, n_embd)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eb25f7",
   "metadata": {},
   "source": [
    "Naturalmente, o uso de uma camada de position embedding `pos_embd` não é de muito uso agora, porque, como estamos usando um modelo bigrama, só o último par de caracteres é considerado. Por outro lado, quando começarmos a trabalhar no mecanismo de atenção, isso vai deixar de ser o caso --- e aí poder ter uma maneira de identificar qual é o último e o penúltimo pares, por exemplo, antes do caractere a ser previsto pode ser muito útil."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b355a4",
   "metadata": {},
   "source": [
    "## 5.5 O mecanismo de auto-atenção\n",
    "\n",
    "O coração de um transformer, a base do GPT, é o mecanismo de atenção (a célula laranja na primeira figura acima)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0e1b507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "B,T,C = 4,8,2 # batch, tempo e caracteres\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2707c792",
   "metadata": {},
   "source": [
    "Nosso objetivo é prever quais caracteres são os mais prováveis; queremos fazer isso prevendo o caractere seguinte usando até 8 caracteres de contexto. A ideia vai ser relacionar cada par de caracteres com um grau de associação,  e depois tomar uma média ponderada. \n",
    "\n",
    "Mas precisamos tomar um cuidado: para prever, digamos, o sexto caractere, podemos usar até os cinco primeiros caracteres --- mas não o sexto, o sétimo e o oitavo. Ou seja, precisamos de uma máscara, e a maneira mais fácil de fazê-lo é simplesmente dar peso zero, na média ponderada. \n",
    "\n",
    "Por ora, vamos simplesmente supor que estamos uma média com peso 1 para cada par."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2190d7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Queremos x[b,t] = média_{i<=t} x[b,i], ou seja a média das associações entre todos os tokens que já apareceram\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)  # Média sobre dimensão t, fica com dimensão C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dda02b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow.shape  # (B, T, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935452d5",
   "metadata": {},
   "source": [
    "Essa versão pode ser melhorada: vários dos cálculos podem ser feitos em paralelo. Uma maneira simples do computador se utilizar dessa possibilidade é escrever tudo matricialmente (isso é particularmente verdade quando estamos usando uma GPU). Nesse caso, o único cuidado é dar peso 1 se o caractere já apareceu na sequência, e 0 caso contrário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbbeae68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(T, T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e463e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Versão 2: usando multiplicação de matrizes\n",
    "weights = torch.tril(torch.ones(T, T))\n",
    "weights = weights / weights.sum(1, keepdim=True)\n",
    "xbow2 = weights @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e23d9b",
   "metadata": {},
   "source": [
    "Ao invés de normalizar, para que os pesos possam ser interpretados como probabilidade (que é o que fizemos acima), nós podemos simplesmente aplicar um softmax. Nesse caso, basta transformar elements onde `tril == 0` em `-inf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09198ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Versão 3: usando softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "weights = torch.zeros((T,T))\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "xbow3 = weights @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09a4dc4",
   "metadata": {},
   "source": [
    "Qual é a vantagem de usar um softmax? É o fato de podemos aprender valores arbitrários para as associações em caracteres em `weights`, inicialmente, e depois deixar `softmax()` transformá-los em probabilidade, como já fazíamos antes.\n",
    "\n",
    "Como aprender os pesos? Basicamente, queremos que os pesos `weights` sejam aprendidos para refletir afinidades entre pares de caracteres. Por exemplo, se olhamos para uma vogal, provavelmente saber quais consoantes aparecem na mesma frase traz mais informação do que as vogais.\n",
    "\n",
    "Vamos fazer isso da seguinte maneira: cada letra vai ter associado um valor em duas matrizes\n",
    " - `key` $\\in \\mathbb{R}^{n_{\\text{embd}} \\times d}$: quem sou eu (e.g., uma vogal). Cada letra (temos `n_embd`) vai ter `d` características.\n",
    "- `query` $\\in \\mathbb{R}^{n_{\\text{embd}} \\times d}$: quais são as informações mais importantes para essa letra querer formar um par (e.g., você é uma consoante?). Cada letra (temos `n_embd`) vai estar interessada em `d` características.\n",
    "\n",
    "A afinidade de um par, `weights`, vai ser dado pelo produto entre a linha correspondente na matrix `query` e o vetor correspondente na matriz `key`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2836234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Versão 4: mecanismo de auto-atenção\n",
    "torch.manual_seed(0)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# Vamos implementar uma Head, isto é, uma maneira de dar atenção a pares\n",
    "head_size = 16  # Quantas características cada letra pode expressar\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "weights =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# weights = torch.zeros((T,T))  # Antes os pesos eram fixos; agora, vão ser aprendidos\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "out = weights @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5acd13",
   "metadata": {},
   "source": [
    "Quem são os nossos pesos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40900f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.8799, 0.1201, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0562, 0.3655, 0.5783, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1704, 0.4478, 0.0397, 0.3421, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2826, 0.1831, 0.0450, 0.3088, 0.1804, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4660, 0.1350, 0.2685, 0.0332, 0.0496, 0.0477, 0.0000, 0.0000],\n",
       "        [0.0463, 0.1805, 0.2533, 0.0089, 0.3825, 0.0815, 0.0468, 0.0000],\n",
       "        [0.0916, 0.2708, 0.0058, 0.2054, 0.0206, 0.0899, 0.2443, 0.0716]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca83c4e9",
   "metadata": {},
   "source": [
    "Ótimo, de fato a afinidade de um caractere na sequência com qualquer outro é zero toda vez que estamos usando um caractere que ainda não apareceu (e.g., na primeira linha, só observamos o primeiro caractere, logo sua afinidade com todos os outros é zero). Esses pares não podem ter qualquer relação.\n",
    "\n",
    "Além disso, através desse mecanismo, a rede consegue detectar quais pares são mais úteis para prever o próximo caractere, mesmo que o par tenha ocorrido bem no começo da sequência --- isso era um problema enorme para redes recorrentes. \n",
    "\n",
    "Valores altos na matriz acima significa que, ao tomar uma média ponderada, pares com muita afinidade recebem peso maior. A única coisa que falta é dar algum grau de flexibilidade aos valores cuja média ponderada vai ser tomada; `x` em si pode não ser a coisa mais útil --- vamos aprender a transformar cada `x` em um `v` tal que, o output do mecanismo de atenção é uma média ponderada dos valores em `v` (e.g., qual é o valor que um par recebe quando um caractere é vogal e outro é uma consoante?). Uma das vantagens de usar essa matriz `v` é que, se quisermos incluir mais mecanismos de atenção, é possível aprender identidades diferentes para `x` simplesmente mudando essa transformação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89c2518f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versão 4: mecanismo de auto-atenção\n",
    "torch.manual_seed(0)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# Vamos implementar uma Head, isto é, uma maneira de dar atenção a pares\n",
    "head_size = 16  # Quantas características cada letra pode expressar\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "weights =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# weights = torch.zeros((T,T))  # Antes os pesos eram fixos; agora, vão ser aprendidos\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
    "weights = weights * (head_size**-0.5)\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "out = weights @ value(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123192b6",
   "metadata": {},
   "source": [
    "Note que a dimensão do output mudou:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03df6733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d55c83",
   "metadata": {},
   "source": [
    "O que aconteceu?\n",
    "- `value = x @ v` $\\in \\mathbb{R}^{B \\times T \\times C} \\times \\mathbb{R}^{C \\times \\text{head\\_size}} = \\mathbb{R}^{B \\times T \\times \\text{head\\_size}}$\n",
    "- `out = weights @ value` $\\in \\mathbb{R}^{B \\times T \\times T} \\times \\mathbb{R}^{B \\times T \\times \\text{head\\_size}} = \\mathbb{R}^{B \\times T \\times \\text{head\\_size}}$, onde a última desigualdade segue de broadcasting (a múltiplicação é feita para cada elemento em $\\mathbb{R}^B$, resultando em $\\mathbb{R}^{T \\times T} \\times \\mathbb{R}^{T \\times \\text{head\\_size}}=\\mathbb{R}^{T \\times T}$).\n",
    "\n",
    "Ou seja, `x` $\\in \\mathbb{R}^{B \\times T \\times C}$, mas `out` $\\in \\mathbb{R}^{B \\times T \\times \\text{head\\_size}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9932a722",
   "metadata": {},
   "source": [
    "Além disso, dividimos os pesos por $1/\\sqrt{\\text{head\\_size}}$ para que possamos inicializar as matrizes `query` e `key` com variância 1 e manter `weights` na mesma escala. Por quê? Vamos pegar um exemplo e entender o que acontece com a variância de `weights`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92268215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance of k: 1.0322861671447754\n",
      "Variance of q: 1.081316351890564\n",
      "Variance of weights: 19.013824462890625\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "weights = q @ k.transpose(-2, -1)\n",
    "\n",
    "print(f\"Variance of k: {k.var()}\")\n",
    "print(f\"Variance of q: {q.var()}\")\n",
    "print(f\"Variance of weights: {weights.var()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd71e67",
   "metadata": {},
   "source": [
    "Qual é o problema da variância aumentar? Nesse caso, o softmax vai acabar favorecendo um único elemento, e a média ponderada passa a se tornar apenas a escolha de um elemento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b04178f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a7c6f9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0228, 0.0015, 0.1382, 0.0015, 0.8359])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]) * 9, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81875f8b",
   "metadata": {},
   "source": [
    "Por isso, normalizamos os pesos por $1/\\sqrt{\\text{head\\_size}}$, e garantimos que a rede conseguirá aprender os pesos corretamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c0437dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance of k: 1.0322861671447754\n",
      "Variance of q: 1.081316351890564\n",
      "Variance of weights: 1.188364028930664\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "weights = q @ k.transpose(-2, -1) * head_size**-0.5\n",
    "\n",
    "print(f\"Variance of k: {k.var()}\")\n",
    "print(f\"Variance of q: {q.var()}\")\n",
    "print(f\"Variance of weights: {weights.var()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cbcc95",
   "metadata": {},
   "source": [
    "**Positional encoding**:  Note que o mecanismo de atenção é uma maneira de dar um grau de plausibilidade a um próximo caractere, onde essa plausibilidade vem de uma média ponderada da associação do próximo caractere proposto e todos os caracteres que já vieram antes. Isso significa que, a princípio, não estamos favorecendo os caracteres que apareceram mais recentemente, algo essencial em linguagem (depois de um \"m\" nós não vamos ter um \"t\", não importa quais outras letras vieram antes). A maneira de resolver isso é colocando o positional encoding, que já estabelecemos preemptivamente. (Repare que a lógica de um mecanismo de atenção é muito diferente de uma convolução, que de fato só dá peso para elementos próximos do elemento sendo considerado.)\n",
    "\n",
    "**Encoder vs decoder**: Finalmente, o que diferencia um encoder de um decoder no caso de um transformer é o fato de que o decoder só pode se utilizar dos elementos na sequência que já apareceram. Um encoder pode se utilizar de todos os elementos. Do ponto de vista de código, a única diferença é deletar a linha `weights = weights.masked_fill(tril == 0, float('-inf'))`. Na prática, treinar um encoder requer muito mais dados que costumamos ter.\n",
    "\n",
    "**Outras opções de atenção**: Estamos usando a expressão auto-atenção às vezes, acima. A razão é que os elementos sempre vêm de `x`, isto é, `key(x)`, `query(x)` e `value(x)`. Existem outros mecanismos de atenção, por exemplo em que as queries vêm de x, mas as keys e values vêm do encoder (veja a primeira figura acima, e como um dos blocos de atenção recebe o output do encoder) ou de uma outra fonte externa de informação (isso é chamado de atenção cruzada)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39069a8",
   "metadata": {},
   "source": [
    "## 5.6 O mecanismo de auto-atenção com multi-Head \n",
    "\n",
    "\n",
    "Tendo visto em detalhes o que é um mecanismo de auto-atenção, vamos implementá-lo de maneira organizada ao nosso código usando uma classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e6c69721",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"Head de um mecanismo de auto-atenção.\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        # Precisamos criar tril de uma maneira diferente, dado que seus parâmetros não são treinados\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input tem tamanho (batch, time-step, channels)\n",
    "        # Output tem tamanho (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # Cálculo dos scores de atenção (\"afinidades\")\n",
    "        weigths = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        weigths = weigths.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        weigths = F.softmax(weigths, dim=-1) # (B, T, T)\n",
    "        # Cálculo da média ponderada dos values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = weigths @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dad3d7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nosso modelo bigrama\n",
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)  # Cada posição recebe um embedding\n",
    "        self.sa_head = Head(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B, T, n_embd)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, n_embd)\n",
    "        x = tok_emb + pos_emb  # (B, T, n_embd)\n",
    "        x = self.sa_head(x)  # (B, T, head_size)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Por conta do positional embedding, o maior contexto que conseguimos receber é\n",
    "            # de tamanho block_size; no caso de gerar vários novos caracteres, precisamos\n",
    "            # restringir o contexto a ter tamanho (máximo) de block_size.\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a68fe8",
   "metadata": {},
   "source": [
    "Que performance essa rede obtém? Por conta da auto-atenção, vamos reduzir um pouco o learning rate e, correspondentemente, aumentar o número de iterações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "852ad358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparâmetros\n",
    "batch_size = 32\n",
    "n_embd = 32\n",
    "block_size = 8\n",
    "max_iters = 5000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a5d51ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteração 0: perda de treino: 3.6880, perda de validação: 3.6867\n",
      "Iteração 300: perda de treino: 2.6155, perda de validação: 2.6582\n",
      "Iteração 600: perda de treino: 2.3757, perda de validação: 2.4427\n",
      "Iteração 900: perda de treino: 2.3186, perda de validação: 2.3772\n",
      "Iteração 1200: perda de treino: 2.3005, perda de validação: 2.3589\n",
      "Iteração 1500: perda de treino: 2.2880, perda de validação: 2.3542\n",
      "Iteração 1800: perda de treino: 2.2634, perda de validação: 2.3477\n",
      "Iteração 2100: perda de treino: 2.2696, perda de validação: 2.3372\n",
      "Iteração 2400: perda de treino: 2.2609, perda de validação: 2.3378\n",
      "Iteração 2700: perda de treino: 2.2565, perda de validação: 2.3241\n",
      "Iteração 3000: perda de treino: 2.2565, perda de validação: 2.3256\n",
      "Iteração 3300: perda de treino: 2.2504, perda de validação: 2.3217\n",
      "Iteração 3600: perda de treino: 2.2394, perda de validação: 2.3194\n",
      "Iteração 3900: perda de treino: 2.2351, perda de validação: 2.3041\n",
      "Iteração 4200: perda de treino: 2.2332, perda de validação: 2.3021\n",
      "Iteração 4500: perda de treino: 2.2278, perda de validação: 2.3045\n",
      "Iteração 4800: perda de treino: 2.2302, perda de validação: 2.3050\n",
      "\n",
      " fzegia. pasumber cro peindo. parta, da no cá-nia tartor, secoi, fisamera ndoia mitroso de cla mpoar-lo alha do que menas de fadeila beravapimumênques mas a ra etumo mos ope, olpra oça vos. foistumotirpelga es esenutantero, apar vereieporve chonsepelos erde ecolh prar lho ra mo mal co, am o quera pelirocisassanosetia o crecomo uria dopelafoa rta eto, co, es,. sadis. se mo e huninestéigoróror diar a ntro não, avess ino, ardgiam e as fidou pamer e ass quentão uimoture so-lartias co, ondo prdeiveins\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(10)\n",
    "\n",
    "model = LanguageModel(vocab_size)\n",
    "model = model.to(device)\n",
    "\n",
    "# Otimizador\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Treinamento\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # De tempos em tempos, imprimir a perda\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Iteração {iter}: perda de treino: {losses['train']:.4f}, perda de validação: {losses['val']:.4f}\")\n",
    "\n",
    "    # Amostre um batch de dados\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Dê um passo na otimização\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Gere alguns caracteres de acordo com o modelo\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(\"\\n\" + decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6129e9fe",
   "metadata": {},
   "source": [
    "Nossa perda de validação caiu de 2.36 para 2.30 --- o mecanismo de atenção parece ter ajudado, mesmo que o texto ainda não se pareça com português. Um problema aqui é que cada auto-atenção só pode dar atenção a uma coisa, mas na verdade a decisão de qual deve ser o próximo caractere pode depender de darmos atenção a mais de um fator ao mesmo tempo. Que tal usar mais de uma Head?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "00165f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Mecanismo de auto-atenção com multi-head.\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        # Várias Heads são criadas e colocadas numa lista\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Concatenamos o resultado na última dimensão e retornamos (B, T, sum(head_sizes))\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7143168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nosso modelo bigrama\n",
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.sa_head = MultiHeadAttention(4, n_embd//4)  # 4 Heads com n_embd=8\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B, T, n_embd)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, n_embd)\n",
    "        x = tok_emb + pos_emb  # (B, T, n_embd)\n",
    "        x = self.sa_head(x)  # (B, T, head_size)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Por conta do positional embedding, o maior contexto que conseguimos receber é\n",
    "            # de tamanho block_size; no caso de gerar vários novos caracteres, precisamos\n",
    "            # restringir o contexto a ter tamanho (máximo) de block_size.\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ee6fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteração 0: perda de treino: 3.7892, perda de validação: 3.7831\n",
      "Iteração 300: perda de treino: 2.5108, perda de validação: 2.5548\n",
      "Iteração 600: perda de treino: 2.3701, perda de validação: 2.4451\n",
      "Iteração 900: perda de treino: 2.3026, perda de validação: 2.3687\n",
      "Iteração 1200: perda de treino: 2.2736, perda de validação: 2.3386\n",
      "Iteração 1500: perda de treino: 2.2558, perda de validação: 2.3233\n",
      "Iteração 1800: perda de treino: 2.2248, perda de validação: 2.3089\n",
      "Iteração 2100: perda de treino: 2.2232, perda de validação: 2.2909\n",
      "Iteração 2400: perda de treino: 2.2063, perda de validação: 2.2891\n",
      "Iteração 2700: perda de treino: 2.1972, perda de validação: 2.2646\n",
      "Iteração 3000: perda de treino: 2.1897, perda de validação: 2.2604\n",
      "Iteração 3300: perda de treino: 2.1793, perda de validação: 2.2552\n",
      "Iteração 3600: perda de treino: 2.1679, perda de validação: 2.2535\n",
      "Iteração 3900: perda de treino: 2.1562, perda de validação: 2.2261\n",
      "Iteração 4200: perda de treino: 2.1492, perda de validação: 2.2240\n",
      "Iteração 4500: perda de treino: 2.1405, perda de validação: 2.2177\n",
      "Iteração 4800: perda de treino: 2.1378, perda de validação: 2.2182\n",
      "\n",
      " fzeria. âpouma o cromos edo. parto, ma no cáloia hatexe, secoi, falheirnándoia mita agrógreva mosar-lme que direconde. as risadeila bentempirue, nãos menga ratetuio mos ope, olpra o asvos. sspátuio preglavass esento um conga não caries ave chonse? endio. latader prúlhas dantelha o com am o que napeção cas de o teuia o crecum huria dopelafontrina, mue se esi. sadas. se mou-mheri estéos, o de diar são mo não maa comino, afogrem e pro cona pamer e asse sontão umaitare. os a tias coma oconço porgo s\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(10)\n",
    "\n",
    "model = LanguageModel(vocab_size)\n",
    "model = model.to(device)\n",
    "\n",
    "# Otimizador\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Treinamento\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # De tempos em tempos, imprimir a perda\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Iteração {iter}: perda de treino: {losses['train']:.4f}, perda de validação: {losses['val']:.4f}\")\n",
    "\n",
    "    # Amostre um batch de dados\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Dê um passo na otimização\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Gere alguns caracteres de acordo com o modelo\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(\"\\n\" + decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd22f3f",
   "metadata": {},
   "source": [
    "Ótimo, nossa perda caiu um pouco mais --- de 2.30 para 2.22. De fato parece que permitir atenção para múltiplos fatores, mesmo que cada um tenha uma dimensão menor, ajuda --- quem são vogais, quem são letras repetidas, quem já apareceu antes no começo da frase, etc. Estamos progredindo, mesmo que o texto ainda não seja bem o que esperamos do Machado de Assis.\n",
    "\n",
    "Existe mais uma coisa que podemos fazer aqui, seguindo a proposta do decoder original: adicionar uma camada feedforward totalmente conectada, depois da atenção e antes da camada linear e do softmax. A razão é permitir que, depois de cada caractere observar aquilo que parece merecer sua atenção, essa camada dá possibilidade dos diferentes caracteres cruzarem informações.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "81fa1e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\"Uma camada linear seguida de uma não-linearidade.\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f8205949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nosso modelo bigrama\n",
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.sa_head = MultiHeadAttention(4, n_embd//4)  # 4 Heads com n_embd=8\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B, T, n_embd)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, n_embd)\n",
    "        x = tok_emb + pos_emb  # (B, T, n_embd)\n",
    "        x = self.sa_head(x)  # (B, T, n_embd)\n",
    "        x = self.ffwd(x) # (B, T, n_embd)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Por conta do positional embedding, o maior contexto que conseguimos receber é\n",
    "            # de tamanho block_size; no caso de gerar vários novos caracteres, precisamos\n",
    "            # restringir o contexto a ter tamanho (máximo) de block_size.\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eddf8c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteração 0: perda de treino: 3.6751, perda de validação: 3.6748\n",
      "Iteração 300: perda de treino: 2.4648, perda de validação: 2.5100\n",
      "Iteração 600: perda de treino: 2.3232, perda de validação: 2.4006\n",
      "Iteração 900: perda de treino: 2.2712, perda de validação: 2.3353\n",
      "Iteração 1200: perda de treino: 2.2401, perda de validação: 2.3089\n",
      "Iteração 1500: perda de treino: 2.2196, perda de validação: 2.2910\n",
      "Iteração 1800: perda de treino: 2.1896, perda de validação: 2.2719\n",
      "Iteração 2100: perda de treino: 2.1896, perda de validação: 2.2603\n",
      "Iteração 2400: perda de treino: 2.1708, perda de validação: 2.2436\n",
      "Iteração 2700: perda de treino: 2.1603, perda de validação: 2.2374\n",
      "Iteração 3000: perda de treino: 2.1638, perda de validação: 2.2397\n",
      "Iteração 3300: perda de treino: 2.1489, perda de validação: 2.2275\n",
      "Iteração 3600: perda de treino: 2.1397, perda de validação: 2.2279\n",
      "Iteração 3900: perda de treino: 2.1324, perda de validação: 2.2156\n",
      "Iteração 4200: perda de treino: 2.1241, perda de validação: 2.2005\n",
      "Iteração 4500: perda de treino: 2.1159, perda de validação: 2.1942\n",
      "Iteração 4800: perda de treino: 2.1164, perda de validação: 2.2006\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(10)\n",
    "\n",
    "model = LanguageModel(vocab_size)\n",
    "model = model.to(device)\n",
    "\n",
    "# Otimizador\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Treinamento\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # De tempos em tempos, imprimir a perda\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Iteração {iter}: perda de treino: {losses['train']:.4f}, perda de validação: {losses['val']:.4f}\")\n",
    "\n",
    "    # Amostre um batch de dados\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Dê um passo na otimização\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6768d1",
   "metadata": {},
   "source": [
    "Mais uma melhora: de 2.22 para 2.20. No total, avançamos até agora de 2.36 para 2.20. \n",
    "\n",
    "Além disso, nossa rede agora tem uma estrutura interessante: o bloco de atenção comunica a relevância de cada par de caracteres, e depois a camada feedforward reúne as informações e calcula o que fazer. Vamos concretizar esse entendimento criando um bloco reunindo essa estrutura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "640eeccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"Block transformer: primeiro, comunicação; depois, cálculo\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sa(x)\n",
    "        x = self.ffwd(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "        )\n",
    "#         self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0666e6a0",
   "metadata": {},
   "source": [
    "Agora, a rede está ficando muito grande, com muitos parâmetros e camadas. Vamos nos valer de três ideias para evitar que isso seja um problema: (i) conexões residuais; (ii) normalização; (iii) dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e95ed18",
   "metadata": {},
   "source": [
    "## 5.7 Conexões residuais\n",
    "\n",
    "Um problema em ter uma rede muito grande é que ficamos muito propensos a ter gradientes explosivos ou dissipados. Isto é, como a regra da cadeia envolve multiplicar termos, acabamos multiplicando vários termos grandes (e o gradiente vai pra infinito) ou pequenos (e o gradiente vai pra zero). No caso de uma ativação como ReLU, aproximadamente metade das vezes o gradiente é exatamente zero, o que eliminaria o aprendizado para alguns pesos.\n",
    "\n",
    "Uma ideia simples para evitar que gradientes se dissipem é criar um caminho direto entre o input e o output (com gradiente um), e adicionar as camadas como um caminho alternativo (no começo as camadas são inicializadas aleatoriamente, portanto contribuem pouco para o aprendizado e têm um gradiente pequeno). Aos poucos o aprendizado começa a acontecer, e a importância desses caminhos alternativos vai aumentando, o que permite que os gradientes se alterem de maneira estável.\n",
    "\n",
    "![](https://i.stack.imgur.com/AdBoF.png)\n",
    "\n",
    "(Uma outra vantagem é que, assim como o positional encoding, o fato do input ser passado adiante até a última camada significa que a ordem dos caracteres também fica disponível em todos os níveis da rede.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "40af4992",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"Block transformer: primeiro, comunicação; depois, cálculo\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "    def forward(self, x):\n",
    "#         x = self.sa(x)\n",
    "        x = x + self.sa(x)\n",
    "#         x = self.ffwd(x)\n",
    "        x = x + self.ffwd(x)\n",
    "        return x\n",
    "# ---------------------------------------------------------------------------------------------------    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98829156",
   "metadata": {},
   "source": [
    "Um problema ao fazer isso é que `x` e a saída de `MultiHeadAttention` e `FeedFoward` precisam ter as mesmas dimensões, para garantir que as operações `x + self.sa(x)` e `x + self.ffwd(x)` sejam válidas. Uma maneira de sempre garantir isso é adicionando uma camada de projeção em cada camada (no nosso caso, ela não é necessária, mas dá mais expressividade ao modelo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bae46be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Mecanismo de auto-atenção com multi-head.\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "# ---------------------------------------------------------------------------------------------------    \n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "# ---------------------------------------------------------------------------------------------------    \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "# ---------------------------------------------------------------------------------------------------    \n",
    "        out = self.proj(out) \n",
    "# ---------------------------------------------------------------------------------------------------    \n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\"Uma camada linear seguida de uma não-linearidade.\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),  # Para seguir o artigo original, dando mais liberdade.\n",
    "            nn.ReLU(),\n",
    "# ---------------------------------------------------------------------------------------------------    \n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "# ---------------------------------------------------------------------------------------------------    \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26123155",
   "metadata": {},
   "source": [
    "Agora, podemos treinar novamente nosso modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e48da35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "        )\n",
    "#         self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Por conta do positional embedding, o maior contexto que conseguimos receber é\n",
    "            # de tamanho block_size; no caso de gerar vários novos caracteres, precisamos\n",
    "            # restringir o contexto a ter tamanho (máximo) de block_size.\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d2fc6fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteração 0: perda de treino: 4.1696, perda de validação: 4.1782\n",
      "Iteração 300: perda de treino: 2.3090, perda de validação: 2.3771\n",
      "Iteração 600: perda de treino: 2.2049, perda de validação: 2.2838\n",
      "Iteração 900: perda de treino: 2.1622, perda de validação: 2.2567\n",
      "Iteração 1200: perda de treino: 2.1195, perda de validação: 2.2166\n",
      "Iteração 1500: perda de treino: 2.0886, perda de validação: 2.1573\n",
      "Iteração 1800: perda de treino: 2.0674, perda de validação: 2.1472\n",
      "Iteração 2100: perda de treino: 2.0489, perda de validação: 2.1291\n",
      "Iteração 2400: perda de treino: 2.0346, perda de validação: 2.1211\n",
      "Iteração 2700: perda de treino: 2.0257, perda de validação: 2.0985\n",
      "Iteração 3000: perda de treino: 2.0055, perda de validação: 2.0932\n",
      "Iteração 3300: perda de treino: 2.0004, perda de validação: 2.0896\n",
      "Iteração 3600: perda de treino: 1.9929, perda de validação: 2.0795\n",
      "Iteração 3900: perda de treino: 1.9774, perda de validação: 2.0750\n",
      "Iteração 4200: perda de treino: 1.9815, perda de validação: 2.0724\n",
      "Iteração 4500: perda de treino: 1.9712, perda de validação: 2.0479\n",
      "Iteração 4800: perda de treino: 1.9672, perda de validação: 2.0517\n"
     ]
    }
   ],
   "source": [
    "n_embd = 32\n",
    "\n",
    "torch.manual_seed(10)\n",
    "\n",
    "model = LanguageModel()\n",
    "model = model.to(device)\n",
    "\n",
    "# Otimizador\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Treinamento\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # De tempos em tempos, imprimir a perda\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Iteração {iter}: perda de treino: {losses['train']:.4f}, perda de validação: {losses['val']:.4f}\")\n",
    "\n",
    "    # Amostre um batch de dados\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Dê um passo na otimização\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12908dd",
   "metadata": {},
   "source": [
    "Fantástico! Fomos de 2.20 para 2.05. Será que agora já estamos perto de português?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "997952be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " mãos., ecoisão da manos é de do a graia dem mordado o dentre minho não antido, mas -lhe com comenterias lesta mesmo que pulio, não genfoite, é cogoração periveiasses era um contas do poco ele predifissõos. fesfar espais jaé nos manhoce. ção é um ele desto re deixa em adesse poi há do dele que tansias do coit, mamentos tam sel, expo da soitor o cooecre arclenes há que fais tenha implete contão. da chora era era exprovoro. de reis. veze combrias que são sangado. pada os vanidades das da zorde? o i\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(\"\\n\" + decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e886367",
   "metadata": {},
   "source": [
    "Hmm, é uma melhora substancial. Ainda faltam uns truques para reduzir ainda mais essa perda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2f2e5d",
   "metadata": {},
   "source": [
    "## 5.8 LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9b496f",
   "metadata": {},
   "source": [
    "Já vimos a ideia de batchnorm e o impacto que essa camada pode ter no treinamento, em particular suavizando os gradientes. No caso de um GPT, os autores utilizaram uma layernorm, que é similar: ao invés de garantir média zero e variância um nos pontos dentro de um batch, vamos fazê-lo dentro da camada. É uma mudança trivial, do ponto de vista do código que tínhamos para `BatchNorm1d`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cae0b229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BatchNorm1d:\n",
    "class LayerNorm1d:\n",
    "  \n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "  \n",
    "    def __call__(self, x):\n",
    "# ---------------------------------------------------------------------------------------------------    \n",
    "#         xmean = x.mean(0, keepdim=True) # média do batch\n",
    "        xmean = x.mean(1, keepdim=True) # média da camada\n",
    "#         xvar = x.var(0, keepdim=True) # variância do batch\n",
    "        xvar = x.var(1, keepdim=True) # variância da camada\n",
    "# ---------------------------------------------------------------------------------------------------    \n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        return self.out\n",
    "  \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f2abcf15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03920de",
   "metadata": {},
   "source": [
    "Ou seja, o batch tem dimensão 32, e para cada elemento do batch as features têm dimensão 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b98824b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.3143), tensor(1.2249))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,0].mean(), x[:,0].std() # média, desvio-padrão de uma feature ao longo de todos os 32 elementos do batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d884e83b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-1.1921e-08), tensor(1.0000))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,:].mean(), x[0,:].std() # média, desvio-padrão de um elemento do batch ao longo de todas as 100 features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efa6e74",
   "metadata": {},
   "source": [
    "(Note que, acima ignormamos as diferenças em modo de treino e de teste: isso era um problema para batchnorm porque o tamanho do batch poderia mudar; o tamanho da camada é sempre o mesmo.)\n",
    "\n",
    "Agora, vamos implementar a rede com layernorm. Uma mudança, em relação à figura inicial acima, é que vamos aplicar a normalização antes da transformação linear, e não depois. Com o tempo, isso se mostrou uma alternativa melhor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cb7cabbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"Block transformer: primeiro, comunicação; depois, cálculo\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "# ---------------------------------------------------------------------------------------------------    \n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "# ---------------------------------------------------------------------------------------------------    \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "# ---------------------------------------------------------------------------------------------------    \n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "# ---------------------------------------------------------------------------------------------------    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bc730c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "            nn.LayerNorm(n_embd)\n",
    "        )\n",
    "# ---------------------------------------------------------------------------------------------------    \n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "# ---------------------------------------------------------------------------------------------------    \n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "        x = self.ln_f(x)\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)    \n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Por conta do positional embedding, o maior contexto que conseguimos receber é\n",
    "            # de tamanho block_size; no caso de gerar vários novos caracteres, precisamos\n",
    "            # restringir o contexto a ter tamanho (máximo) de block_size.\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8973d9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteração 0: perda de treino: 3.9061, perda de validação: 3.9092\n",
      "Iteração 300: perda de treino: 2.3166, perda de validação: 2.3758\n",
      "Iteração 600: perda de treino: 2.2150, perda de validação: 2.2881\n",
      "Iteração 900: perda de treino: 2.1563, perda de validação: 2.2468\n",
      "Iteração 1200: perda de treino: 2.1244, perda de validação: 2.2144\n",
      "Iteração 1500: perda de treino: 2.0950, perda de validação: 2.1654\n",
      "Iteração 1800: perda de treino: 2.0698, perda de validação: 2.1511\n",
      "Iteração 2100: perda de treino: 2.0575, perda de validação: 2.1429\n",
      "Iteração 2400: perda de treino: 2.0385, perda de validação: 2.1265\n",
      "Iteração 2700: perda de treino: 2.0332, perda de validação: 2.1049\n",
      "Iteração 3000: perda de treino: 2.0075, perda de validação: 2.0978\n",
      "Iteração 3300: perda de treino: 2.0072, perda de validação: 2.0963\n",
      "Iteração 3600: perda de treino: 1.9885, perda de validação: 2.0729\n",
      "Iteração 3900: perda de treino: 1.9744, perda de validação: 2.0694\n",
      "Iteração 4200: perda de treino: 1.9756, perda de validação: 2.0674\n",
      "Iteração 4500: perda de treino: 1.9677, perda de validação: 2.0457\n",
      "Iteração 4800: perda de treino: 1.9633, perda de validação: 2.0433\n"
     ]
    }
   ],
   "source": [
    "n_embd = 32\n",
    "\n",
    "torch.manual_seed(10)\n",
    "\n",
    "model = LanguageModel()\n",
    "model = model.to(device)\n",
    "\n",
    "# Otimizador\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Treinamento\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # De tempos em tempos, imprimir a perda\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Iteração {iter}: perda de treino: {losses['train']:.4f}, perda de validação: {losses['val']:.4f}\")\n",
    "\n",
    "    # Amostre um batch de dados\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Dê um passo na otimização\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21fc33a",
   "metadata": {},
   "source": [
    "Hmm, parece que a layernorm não nos ajudou tanto: fomos de 2.0517 para 2.0433. Mas, por outro lado, repare que estamos com a perda de treino muito abaixo da de validação. Talvez devéssemos nos preocupar em reduzir o overfitting. Uma solução é o dropout --- vamos adicioná-lo em algumas camadas. Na mesma medida em que vamos nos preocupar em prevenir overfitting, podemos aumentar a escala do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af2fab6",
   "metadata": {},
   "source": [
    "## 5.8 Modelo final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9eaf5346",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "block_size = 256 # tamanho do contexto agora é muito maior!\n",
    "max_iters = 15000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4 # 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 8\n",
    "n_layer = 8\n",
    "dropout = 0.2\n",
    "\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a9ebc2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"Head de um mecanismo de auto-atenção.\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        weights = q @ k.transpose(-2,-1) * C**-0.5\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "        weights = self.dropout(weights)  # Algumas conexões aleatoriamente deixam de ser usadas por robustez\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "        v = self.value(x)\n",
    "        out = weights @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Mecanismo de auto-atenção com multi-head.\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "        out = self.dropout(self.proj(out))\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\"Uma camada linear seguida de uma não-linearidade.\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "            nn.Dropout(dropout),\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Block transformer: primeiro, comunicação; depois, cálculo\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b224927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDecoderModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e1e711d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de parâmetros: 14.317866M\n"
     ]
    }
   ],
   "source": [
    "model = GPTDecoderModel()\n",
    "m = model.to(device)\n",
    "print(f\"Número de parâmetros: {sum(p.numel() for p in m.parameters())/1e6}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "777f2229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# for iter in range(max_iters):\n",
    "\n",
    "#     if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "#         losses = estimate_loss()\n",
    "#         print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "#     xb, yb = get_batch('train')\n",
    "\n",
    "#     logits, loss = model(xb, yb)\n",
    "#     optimizer.zero_grad(set_to_none=True)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8964ecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 0: train loss 3.8188, val loss 3.8246\n",
    "# step 500: train loss 2.1604, val loss 2.2320\n",
    "# step 1000: train loss 1.7055, val loss 1.8254\n",
    "# step 1500: train loss 1.5085, val loss 1.6635\n",
    "# step 2000: train loss 1.4085, val loss 1.5826\n",
    "# step 2500: train loss 1.3511, val loss 1.5290\n",
    "# step 3000: train loss 1.3079, val loss 1.4974\n",
    "# step 3500: train loss 1.2755, val loss 1.4709\n",
    "# step 4000: train loss 1.2579, val loss 1.4581\n",
    "# step 4500: train loss 1.2368, val loss 1.4317\n",
    "# step 5000: train loss 1.2192, val loss 1.4123\n",
    "# step 5500: train loss 1.2053, val loss 1.4128\n",
    "# step 6000: train loss 1.1920, val loss 1.3997\n",
    "# step 6500: train loss 1.1815, val loss 1.3830\n",
    "# step 7000: train loss 1.1688, val loss 1.3781\n",
    "# step 7500: train loss 1.1600, val loss 1.3645\n",
    "# step 8000: train loss 1.1539, val loss 1.3610\n",
    "# step 8500: train loss 1.1440, val loss 1.3552\n",
    "# step 9000: train loss 1.1324, val loss 1.3372\n",
    "# step 9500: train loss 1.1286, val loss 1.3373\n",
    "# step 10000: train loss 1.1215, val loss 1.3298\n",
    "# step 10500: train loss 1.1136, val loss 1.3237\n",
    "# step 11000: train loss 1.1096, val loss 1.3214\n",
    "# step 11500: train loss 1.1039, val loss 1.3160\n",
    "# step 12000: train loss 1.1019, val loss 1.3158\n",
    "# step 12500: train loss 1.0944, val loss 1.3139\n",
    "# step 13000: train loss 1.0922, val loss 1.3121\n",
    "# step 13500: train loss 1.0886, val loss 1.3101\n",
    "# step 14000: train loss 1.0815, val loss 1.3078\n",
    "# step 14500: train loss 1.0799, val loss 1.3054\n",
    "# step 14999: train loss 1.0754, val loss 1.3058"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906fd8ba",
   "metadata": {},
   "source": [
    "Antes, com redes neurais de muitas camadas, estávamos em ~1.7. Em algumas iterações, já estamos em 1.3! Por outro lado, a rede é muito mais lenta, e repare que a escala que adicionamos é fundamental: antes, o melhor que conseguimos com nossa rede GPTDecoderModel foi 2.04!\n",
    "\n",
    "Será que agora, finalmente, temos algo que parece Machado de Assis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "708c6fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(10);\n",
    "# context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "# print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5dc472",
   "metadata": {},
   "source": [
    "_uma série de objetos que precisasse da música de madri e da situação não de vida oficial. só isso? acho um súdito relíquido isto não teve pressa. seu óculo, falo do que eu vi casar-se ama-me, e não menos do que eu quincassibasse tanto ou fazia calar o tempo depois. seste ano para sábio, e os últimos primeiros tantos anos, quem ali nho mundo, onde ama? olhe, quem tem saídas? ó filha. ia a cortesia do primeiro, queres, posto quem visse uma página de terra, pouca vez tentava passo de amar para outr_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
